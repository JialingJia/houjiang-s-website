@INPROCEEDINGS{Huang2022-qh,
  title     = "Faking Fake News for Real Fake News Detection:
               {Propaganda-Loaded} Training Data Generation",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  author    = "Huang, Kung-Hsiang and McKeown, Kathleen and Nakov, Preslav and
               Choi, Yejin and Ji, Heng",
  editor    = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
  abstract  = "Despite recent advances in detecting fake news generated by
               neural models, their results are not readily applicable to
               effective detection of human-written disinformation. What limits
               the successful transfer between them is the sizable gap between
               machine-generated fake news and human-authored ones, including
               the notable differences in terms of style and underlying intent.
               With this in mind, we propose a novel framework for generating
               training examples that are informed by the known styles and
               strategies of human-authored propaganda. Specifically, we
               perform self-critical sequence training guided by natural
               language inference to ensure the validity of the generated
               articles, while also incorporating propaganda techniques, such
               as appeal to authority and loaded language. In particular, we
               create a new training dataset, PropaNews, with 2,256 examples,
               which we release for future use. Our experimental results show
               that fake news detectors trained on PropaNews are better at
               detecting human-written disinformation by 3.62--7.69\% F1 score
               on two public datasets.",
  publisher = "Association for Computational Linguistics",
  pages     = "14571--14589",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  doi       = "10.18653/v1/2023.acl-long.815"
}

@INPROCEEDINGS{Pan2023-me,
  title     = "On the Risk of Misinformation Pollution with Large Language
               Models",
  booktitle = "Findings of the Association for Computational Linguistics:
               {EMNLP} 2023",
  author    = "Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov,
               Preslav and Kan, Min-Yen and Wang, William",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  abstract  = "We investigate the potential misuse of modern Large Language
               Models (LLMs) for generating credible-sounding misinformation
               and its subsequent impact on information-intensive applications,
               particularly Open-Domain Question Answering (ODQA) systems. We
               establish a threat model and simulate potential misuse
               scenarios, both unintentional and intentional, to assess the
               extent to which LLMs can be utilized to produce misinformation.
               Our study reveals that LLMs can act as effective misinformation
               generators, leading to a significant degradation (up to 87\%) in
               the performance of ODQA systems. Moreover, we uncover
               disparities in the attributes associated with persuading humans
               and machines, presenting an obstacle to current human-centric
               approaches to combat misinformation. To mitigate the harm caused
               by LLM-generated misinformation, we propose three defense
               strategies: misinformation detection, vigilant prompting, and
               reader ensemble. These approaches have demonstrated promising
               results, albeit with certain associated costs. Lastly, we
               discuss the practicality of utilizing LLMs as automatic
               misinformation generators and provide relevant resources and
               code to facilitate future research in this area.",
  publisher = "Association for Computational Linguistics",
  pages     = "1389--1403",
  month     =  dec,
  year      =  2023,
  address   = "Singapore",
  doi       = "10.18653/v1/2023.findings-emnlp.97"
}

@INPROCEEDINGS{Jiang2021-ef,
  title     = "Structurizing Misinformation Stories via Rationalizing
               {Fact-Checks}",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for
               Computational Linguistics and the 11th International Joint
               Conference on Natural Language Processing (Volume 1: Long
               Papers)",
  author    = "Jiang, Shan and Wilson, Christo",
  editor    = "Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto",
  abstract  = "Misinformation has recently become a well-documented matter of
               public concern. Existing studies on this topic have hitherto
               adopted a coarse concept of misinformation, which incorporates a
               broad spectrum of story types ranging from political
               conspiracies to misinterpreted pranks. This paper aims to
               structurize these misinformation stories by leveraging
               fact-check articles. Our intuition is that key phrases in a
               fact-check article that identify the misinformation type(s)
               (e.g., doctored images, urban legends) also act as rationales
               that determine the verdict of the fact-check (e.g., false). We
               experiment on rationalized models with domain knowledge as weak
               supervision to extract these phrases as rationales, and then
               cluster semantically similar rationales to summarize prevalent
               misinformation types. Using archived fact-checks from
               Snopes.com, we identify ten types of misinformation stories. We
               discuss how these types have evolved over the last ten years and
               compare their prevalence between the 2016/2020 US presidential
               elections and the H1N1/COVID-19 pandemics.",
  publisher = "Association for Computational Linguistics",
  pages     = "617--631",
  month     =  aug,
  year      =  2021,
  address   = "Online",
  doi       = "10.18653/v1/2021.acl-long.51"
}

@INPROCEEDINGS{Hardalov2021-zm,
  title     = "A Survey on Stance Detection for Mis- and Disinformation
               Identification",
  booktitle = "Findings of the Association for Computational Linguistics:
               {NAACL} 2022",
  author    = "Hardalov, Momchil and Arora, Arnav and Nakov, Preslav and
               Augenstein, Isabelle",
  editor    = "Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz,
               Ivan Vladimir",
  abstract  = "Understanding attitudes expressed in texts, also known as stance
               detection, plays an important role in systems for detecting
               false information online, be it misinformation (unintentionally
               false) or disinformation (intentionally false information).
               Stance detection has been framed in different ways, including
               (a) as a component of fact-checking, rumour detection, and
               detecting previously fact-checked claims, or (b) as a task in
               its own right. While there have been prior efforts to contrast
               stance detection with other related tasks such as argumentation
               mining and sentiment analysis, there is no existing survey on
               examining the relationship between stance detection and mis- and
               disinformation detection. Here, we aim to bridge this gap by
               reviewing and analysing existing work in this area, with mis-
               and disinformation in focus, and discussing lessons learnt and
               future challenges.",
  publisher = "Association for Computational Linguistics",
  pages     = "1259--1277",
  month     =  jul,
  year      =  2022,
  address   = "Seattle, United States",
  doi       = "10.18653/v1/2022.findings-naacl.94"
}

@INPROCEEDINGS{Arakelyan2023-ju,
  title     = "{Topic-Guided} Sampling For {Data-Efficient} {Multi-Domain}
               Stance Detection",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  author    = "Arakelyan, Erik and Arora, Arnav and Augenstein, Isabelle",
  editor    = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
  abstract  = "The task of Stance Detection is concerned with identifying the
               attitudes expressed by an author towards a target of interest.
               This task spans a variety of domains ranging from social media
               opinion identification to detecting the stance for a legal
               claim. However, the framing of the task varies within these
               domains in terms of the data collection protocol, the label
               dictionary and the number of available annotations. Furthermore,
               these stance annotations are significantly imbalanced on a
               per-topic and inter-topic basis. These make multi-domain stance
               detection challenging, requiring standardization and domain
               adaptation. To overcome this challenge, we propose Topic
               Efficient StancE Detection (TESTED), consisting of a
               topic-guided diversity sampling technique used for creating a
               multi-domain data efficient training set and a contrastive
               objective that is used for fine-tuning a stance classifier using
               the produced set. We evaluate the method on an existing
               benchmark of 16 datasets with in-domain, i.e. all topics seen
               and out-of-domain, i.e. unseen topics, experiments. The results
               show that the method outperforms the state-of-the-art with an
               average of 3.5 F1 points increase in-domain and is more
               generalizable with an averaged 10.2 F1 on out-of-domain
               evaluation while using \textbackslashtextless10\% of the
               training data. We show that our sampling technique mitigates
               both inter- and per-topic class imbalances. Finally, our
               analysis demonstrates that the contrastive learning objective
               allows the model for a more pronounced segmentation of samples
               with varying labels.",
  publisher = "Association for Computational Linguistics",
  pages     = "13448--13464",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  doi       = "10.18653/v1/2023.acl-long.752"
}

@ARTICLE{zhou2020survey,
  title     = "A Survey of Fake News: Fundamental Theories, Detection Methods,
               and Opportunities",
  author    = "Zhou, Xinyi and Zafarani, Reza",
  abstract  = "The explosive growth in fake news and its erosion to democracy,
               justice, and public trust has increased the demand for fake news
               detection and intervention. This survey reviews and evaluates
               methods that can detect fake news from four perspectives: the
               false knowledge it carries, its writing style, its propagation
               patterns, and the credibility of its source. The survey also
               highlights some potential research tasks based on the review. In
               particular, we identify and detail related fundamental theories
               across various disciplines to encourage interdisciplinary
               research on fake news. It is our hope that this survey can
               facilitate collaborative efforts among experts in computer and
               information sciences, social sciences, political science, and
               journalism to research fake news, where such efforts can lead to
               fake news detection that is not only efficient but, more
               importantly, explainable.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--40",
  month     =  "28~" # sep,
  year      =  2020,
  address   = "New York, NY, USA",
  keywords  = "Fake news, deception detection, disinformation, fact-checking,
               information credibility, knowledge graph, misinformation, news
               verification, social media",
  issn      = "0360-0300",
  doi       = "10.1145/3395046"
}

@INPROCEEDINGS{Nguyen2018believeornot,
  title     = "Believe it or not: Designing a {Human-AI} Partnership for
               {Mixed-Initiative} {Fact-Checking}",
  booktitle = "Proceedings of the 31st Annual {ACM} Symposium on User Interface
               Software and Technology",
  author    = "Nguyen, An T and Kharosekar, Aditya and Krishnan, Saumyaa and
               Krishnan, Siddhesh and Tate, Elizabeth and Wallace, Byron C and
               Lease, Matthew",
  abstract  = "Fact-checking, the task of assessing the veracity of claims, is
               an important, timely, and challenging problem. While many
               automated fact-checking systems have been recently proposed, the
               human side of the partnership has been largely neglected: how
               might people understand, interact with, and establish trust with
               an AI fact-checking system? Does such a system actually help
               people better assess the factuality of claims? In this paper, we
               present the design and evaluation of a mixed-initiative approach
               to fact-checking, blending human knowledge and experience with
               the efficiency and scalability of automated information
               retrieval and ML. In a user study in which participants used our
               system to aid their own assessment of claims, our results
               suggest that individuals tend to trust the system: participant
               accuracy assessing claims improved when exposed to correct model
               predictions. However, this trust perhaps goes too far: when the
               model was wrong, exposure to its predictions often degraded
               human accuracy. Participants given the option to interact with
               these incorrect predictions were often able improve their own
               performance. This suggests that transparent models are key to
               facilitating effective human interaction with fallible AI
               models.",
  publisher = "Association for Computing Machinery",
  pages     = "189--199",
  series    = "UIST '18",
  month     =  "11~" # oct,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "mixed-initiative, information literacy, fact-checking, AI",
  location  = "Berlin, Germany",
  isbn      = "9781450359481",
  doi       = "10.1145/3242587.3242666"
}

@INPROCEEDINGS{Giachanou2019-qf,
  title     = "Leveraging Emotional Signals for Credibility Detection",
  booktitle = "Proceedings of the 42nd International {ACM} {SIGIR} Conference
               on Research and Development in Information Retrieval",
  author    = "Giachanou, Anastasia and Rosso, Paolo and Crestani, Fabio",
  abstract  = "The spread of false information on the Web is one of the main
               problems of our society. Automatic detection of fake news posts
               is a hard task since they are intentionally written to mislead
               the readers and to trigger intense emotions to them in an
               attempt to be disseminated in the social networks. Even though
               recent studies have explored different linguistic patterns of
               false claims, the role of emotional signals has not yet been
               explored. In this paper, we study the role of emotional signals
               in fake news detection. In particular, we propose an LSTM model
               that incorporates emotional signals extracted from the text of
               the claims to differentiate between credible and non-credible
               ones. Experiments on real world datasets show the importance of
               emotional signals for credibility assessment.",
  publisher = "Association for Computing Machinery",
  pages     = "877--880",
  series    = "SIGIR'19",
  month     =  "18~" # jul,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "Credibility detection; Emotional signals; Fake news detection;
               LSTM",
  location  = "Paris, France",
  isbn      = "9781450361729",
  doi       = "10.1145/3331184.3331285"
}

@INPROCEEDINGS{Zhi2017-wv,
  title     = "{ClaimVerif}: A Real-time Claim Verification System Using the
               Web and Fact Databases",
  booktitle = "Proceedings of the 2017 {ACM} on Conference on Information and
               Knowledge Management",
  author    = "Zhi, Shi and Sun, Yicheng and Liu, Jiayi and Zhang, Chao and
               Han, Jiawei",
  abstract  = "Our society is increasingly digitalized. Every day, a tremendous
               amount of information is being created, shared, and digested
               through all kinds of cyber channels. Although people can easily
               acquire information from various sources (social media, news
               articles, etc.), the truthfulness of most received information
               remains unverified. In many real-life scenarios, false
               information has become the de facto cause that leads to
               detrimental decision makings, and techniques that can
               automatically filter false information are highly demanded.
               However, verifying whether a piece of information is trustworthy
               is difficult because: (1) selecting candidate snippets for fact
               checking is nontrivial; and (2) detecting supporting evidences,
               i.e. stances, suffers from the difficulty of measuring the
               similarity between claims and related evidences. We build
               ClaimVerif, a claim verification system that not only provides
               credibility assessment for any user-given query claim, but also
               rationales the assessment results with supporting evidences.
               ClaimVerif can automatically select the stances from millions of
               documents and employs two-step training to justify the opinions
               of the stances. Furthermore, combined with the credibility of
               stances sources, ClaimVerif degrades the score of stances from
               untrustworthy sources and alleviates the negative effects from
               rumor spreaders. Our empirical evaluations show that ClaimVerif
               achieves both high accuracy and efficiency in different claim
               verification tasks. It can be highly useful in practical
               applications by providing multi-dimension analysis for the
               suspicious statements, including the stances, opinions, source
               credibility and estimated judgements.",
  publisher = "Association for Computing Machinery",
  volume    = "Part F1318",
  pages     = "2555--2558",
  series    = "CIKM '17",
  month     =  "6~" # nov,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "rumor detection, text mining, fact checking, source credibility
               analysis",
  location  = "Singapore, Singapore",
  issn      = "2155-0751",
  isbn      = "9781450349185",
  doi       = "10.1145/3132847.3133182"
}

@INPROCEEDINGS{Rashkin2017-ct,
  title     = "Truth of Varying Shades: Analyzing Language in Fake News and
               Political {Fact-Checking}",
  booktitle = "Proceedings of the 2017 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Rashkin, Hannah and Choi, Eunsol and Jang, Jin Yea and Volkova,
               Svitlana and Choi, Yejin",
  editor    = "Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian",
  abstract  = "We present an analytic study on the language of news media in
               the context of political fact-checking and fake news detection.
               We compare the language of real news with that of satire,
               hoaxes, and propaganda to find linguistic characteristics of
               untrustworthy text. To probe the feasibility of automatic
               political fact-checking, we also present a case study based on
               PolitiFact.com using their factuality judgments on a 6-point
               scale. Experiments show that while media fact-checking remains
               to be an open research question, stylistic cues can help
               determine the truthfulness of text.",
  publisher = "Association for Computational Linguistics",
  pages     = "2931--2937",
  month     =  sep,
  year      =  2017,
  address   = "Copenhagen, Denmark",
  doi       = "10.18653/v1/D17-1317"
}

@ARTICLE{Choi2021-bm,
  title    = "Decontextualization: Making sentences stand-alone",
  author   = "Choi, Eunsol and Palomaki, Jennimaria and Lamm, Matthew and
              Kwiatkowski, Tom and Das, Dipanjan and Collins, Michael",
  abstract = "Models for question answering, dialogue agents, and summarization
              often interpret the meaning of a sentence in a rich context and
              use that meaning in a new context. Taking excerpts of text can be
              problematic, as key pieces may not be explicit in a local window.
              We isolate and define the problem of sentence
              decontextualization: taking a sentence together with its context
              and rewriting it to be interpretable out of context, while
              preserving its meaning. We describe an annotation procedure,
              collect data on the Wikipedia corpus, and use the data to train
              models to automatically decontextualize sentences. We present
              preliminary studies that show the value of sentence
              decontextualization in a user-facing task, and as preprocessing
              for systems that perform document understanding. We argue that
              decontextualization is an important subtask in many downstream
              applications, and that the definitions and resources provided can
              benefit tasks that operate on sentences that occur in a richer
              context.",
  journal  = "Transactions of the Association for Computational Linguistics",
  volume   =  9,
  pages    = "447--461",
  year     =  2021,
  issn     = "2307-387X",
  arxivid  = "2102.05169",
  doi      = "10.1162/tacl_a_00377"
}

@INPROCEEDINGS{Chen2022-yz,
  title     = "Generating Literal and Implied Subquestions to Fact-check
               Complex Claims",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Chen, Jifan and Sriram, Aniruddh and Choi, Eunsol and Durrett,
               Greg",
  editor    = "Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue",
  abstract  = "Verifying political claims is a challenging task, as politicians
               can use various tactics to subtly misrepresent the facts for
               their agenda. Existing automatic fact-checking systems fall
               short here, and their predictions like ``half-true'' are not
               very useful in isolation, since it is unclear which parts of a
               claim are true and which are not. In this work, we focus on
               decomposing a complex claim into a comprehensive set of yes-no
               subquestions whose answers influence the veracity of the claim.
               We present CLAIMDECOMP, a dataset of decompositions for over
               1000 claims. Given a claim and its verification paragraph
               written by fact-checkers, our trained annotators write
               subquestions covering both explicit propositions of the original
               claim and its implicit facets, such as asking about additional
               political context that changes our view of the claim's veracity.
               We study whether state-of-the-art models can generate such
               subquestions, showing that these models generate reasonable
               questions to ask, but predicting the comprehensive set of
               subquestions from the original claim without evidence remains
               challenging. We further show that these subquestions can help
               identify relevant evidence to fact-check the full claim and
               derive the veracity through their answers, suggesting that they
               can be useful pieces of a fact-checking pipeline.",
  publisher = "Association for Computational Linguistics",
  pages     = "3495--3516",
  month     =  dec,
  year      =  2022,
  address   = "Abu Dhabi, United Arab Emirates",
  doi       = "10.18653/v1/2022.emnlp-main.229"
}

@INPROCEEDINGS{alam2020fighting,
  title     = "Fighting the {{COVID}-19} Infodemic: Modeling the Perspective of
               Journalists, {Fact-Checkers}, Social Media Platforms, Policy
               Makers, and the Society",
  booktitle = "Findings of the Association for Computational Linguistics:
               {EMNLP} 2021",
  author    = "Alam, Firoj and Shaar, Shaden and Dalvi, Fahim and Sajjad,
               Hassan and Nikolov, Alex and Mubarak, Hamdy and Da San Martino,
               Giovanni and Abdelali, Ahmed and Durrani, Nadir and Darwish,
               Kareem and Al-Homaid, Abdulaziz and Zaghouani, Wajdi and
               Caselli, Tommaso and Danoe, Gijs and Stolk, Friso and Bruntink,
               Britt and Nakov, Preslav",
  editor    = "Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and
               Yih, Scott Wen-Tau",
  abstract  = "With the emergence of the COVID-19 pandemic, the political and
               the medical aspects of disinformation merged as the problem got
               elevated to a whole new level to become the first global
               infodemic. Fighting this infodemic has been declared one of the
               most important focus areas of the World Health Organization,
               with dangers ranging from promoting fake cures, rumors, and
               conspiracy theories to spreading xenophobia and panic.
               Addressing the issue requires solving a number of challenging
               problems such as identifying messages containing claims,
               determining their check-worthiness and factuality, and their
               potential to do harm as well as the nature of that harm, to
               mention just a few. To address this gap, we release a large
               dataset of 16K manually annotated tweets for fine-grained
               disinformation analysis that (i) focuses on COVID-19, (ii)
               combines the perspectives and the interests of journalists,
               fact-checkers, social media platforms, policy makers, and
               society, and (iii) covers Arabic, Bulgarian, Dutch, and English.
               Finally, we show strong evaluation results using pretrained
               Transformers, thus confirming the practical utility of the
               dataset in monolingual vs. multilingual, and single task vs.
               multitask settings.",
  publisher = "Association for Computational Linguistics",
  pages     = "611--649",
  month     =  nov,
  year      =  2021,
  address   = "Punta Cana, Dominican Republic",
  doi       = "10.18653/v1/2021.findings-emnlp.56"
}

@ARTICLE{Hassan2017TowardAF,
  title    = "Overview of the {CLEF-2021} {CheckThat}! Lab task 1 on
              check-worthiness estimation in tweets and political debates",
  author   = "Shaar, S and Hasanain, Maram and Hamdan, Bayan and Ali, Zien
              Sheikh and Haouari, Fatima and Nikolov, Alex and Kutlu, Mucahid
              and Kartal, Yavuz Selim and Alam, Firoj and Da San Martino,
              Giovanni and Barr{\'o}n-Cede{\~n}o, Alberto and M{\'\i}guez,
              Rub{\'e}n and Beltr{\'a}n, Javier and Elsayed, Tamer and Nakov,
              Preslav",
  abstract = "We present an overview of Task 1 of the fourth edition of the
              CheckThat! Lab, part of the 2021 Conference and Labs of the
              Evaluation Forum (CLEF). The task asks to predict which posts in
              a Twitter stream are worth fact-checking, focusing on COVID-19
              and politics in five languages: Arabic, Bulgarian, English,
              Spanish, and Turkish. A total of 15 teams participated in this
              task and most submissions managed to achieve sizable improvements
              over the baselines using Transformer-based models such as BERT
              and RoBERTa. Here, we describe the process of data collection and
              the task setup, including the evaluation measures, and we give a
              brief overview of the participating systems. We release to the
              research community all datasets from the lab as well as the
              evaluation scripts, which should enable further research in
              check-worthiness estimation for tweets and political debates.
              \copyright{} 2021 Copyright for this paper by its authors. Use
              permitted under Creative Commons License Attribution 4.0
              International (CC BY 4.0).",
  journal  = "Conference and Labs of the Evaluation Forum",
  pages    = "369--392",
  year     =  2021,
  doi      = "https://ceur-ws.org/Vol-2936/paper-28.pdf"
}

@INPROCEEDINGS{vasileva2019takes,
  title     = "It Takes Nine to Smell a Rat: Neural {Multi-Task} Learning for
               {Check-Worthiness} Prediction",
  booktitle = "Proceedings of the International Conference on Recent Advances
               in Natural Language Processing ({RANLP} 2019)",
  author    = "Vasileva, Slavena and Atanasova, Pepa and M{\`a}rquez,
               Llu{\'\i}s and Barr{\'o}n-Cede{\~n}o, Alberto and Nakov, Preslav",
  editor    = "Mitkov, Ruslan and Angelova, Galia",
  abstract  = "We propose a multi-task deep-learning approach for estimating
               the check-worthiness of claims in political debates. Given a
               political debate, such as the 2016 US Presidential and
               Vice-Presidential ones, the task is to predict which statements
               in the debate should be prioritized for fact-checking. While
               different fact-checking organizations would naturally make
               different choices when analyzing the same debate, we show that
               it pays to learn from multiple sources simultaneously
               (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The
               Guardian, and Washington Post) in a multi-task learning setup,
               even when a particular source is chosen as a target to imitate.
               Our evaluation shows state-of-the-art results on a standard
               dataset for the task of check-worthiness prediction.",
  publisher = "INCOMA Ltd.",
  pages     = "1229--1239",
  month     =  sep,
  year      =  2019,
  address   = "Varna, Bulgaria",
  doi       = "10.26615/978-954-452-056-4_141"
}

@INPROCEEDINGS{Nakov2022TheCC,
  title     = "The {CLEF-2022} {CheckThat}! Lab on Fighting the {COVID-19}
               Infodemic and Fake News Detection",
  booktitle = "Advances in Information Retrieval",
  author    = "Nakov, Preslav and Barr{\'o}n-Cede{\~n}o, Alberto and Da San
               Martino, Giovanni and Alam, Firoj and Stru{\ss}, Julia Maria and
               Mandl, Thomas and M{\'\i}guez, Rub{\'e}n and Caselli, Tommaso
               and Kutlu, Mucahid and Zaghouani, Wajdi and Li, Chengkai and
               Shaar, Shaden and Shahi, Gautam Kishore and Mubarak, Hamdy and
               Nikolov, Alex and Babulkov, Nikolay and Kartal, Yavuz Selim and
               Beltr{\'a}n, Javier",
  abstract  = "The fifth edition of the CheckThat! Lab is held as part of the
               2022 Conference and Labs of the Evaluation Forum (CLEF). The lab
               evaluates technology supporting various factuality tasks in
               seven languages: Arabic, Bulgarian, Dutch, English, German,
               Spanish, and Turkish. Task 1 focuses on disinformation related
               to the ongoing COVID-19 infodemic and politics, and asks to
               predict whether a tweet is worth fact-checking, contains a
               verifiable factual claim, is harmful to the society, or is of
               interest to policy makers and why. Task 2 asks to retrieve
               claims that have been previously fact-checked and that could be
               useful to verify the claim in a tweet. Task 3 is to predict the
               veracity of a news article. Tasks 1 and 3 are classification
               problems, while Task 2 is a ranking one.",
  publisher = "Springer International Publishing",
  pages     = "416--428",
  year      =  2022,
  doi       = "10.1007/978-3-031-13643-6_29"
}

@ARTICLE{alam2021fighting,
  title    = "Fighting the {COVID-19} Infodemic in Social Media: A Holistic
              Perspective and a Call to Arms",
  author   = "Alam, Firoj and Dalvi, Fahim and Shaar, Shaden and Durrani, Nadir
              and Mubarak, Hamdy and Nikolov, Alex and Da San Martino, Giovanni
              and Abdelali, Ahmed and Sajjad, Hassan and Darwish, Kareem and
              Nakov, Preslav",
  abstract = "With the outbreak of the COVID-19 pandemic, people turned to
              social media to read and to share timely information including
              statistics, warnings, advice, and inspirational stories.
              Unfortunately, alongside all this useful information, there was
              also a new blending of medical and political misinformation and
              disinformation, which gave rise to the first global infodemic.
              While fighting this infodemic is typically thought of in terms of
              factuality, the problem is much broader as malicious content
              includes not only fake news, rumors, and conspiracy theories, but
              also promotion of fake cures, panic, racism, xenophobia, and
              mistrust in the authorities, among others. This is a complex
              problem that needs a holistic approach combining the perspectives
              of journalists, fact-checkers, policymakers, government entities,
              social media platforms, and society as a whole. With this in
              mind, we define an annotation schema and detailed annotation
              instructions that reflect these perspectives. We further deploy a
              multilingual annotation platform, and we issue a call to arms to
              the research community and beyond to join the fight by supporting
              our crowdsourcing annotation efforts. We perform initial
              annotations using the annotation schema, and our initial
              experiments demonstrated sizable improvements over the baselines.",
  journal  = "Proceedings of the International AAAI Conference on Web and
              Social Media",
  volume   =  15,
  number   =  1,
  pages    = "913--922",
  month    =  "22~" # may,
  year     =  2021,
  keywords = "Credibility of online content; Text categorization; topic
              recognition; demographic/gender/age identification",
  language = "en",
  issn     = "2334-0770, 2334-0770",
  doi      = "10.1609/icwsm.v15i1.18114"
}

@ARTICLE{Kartal2020-hp,
  title         = "Too Many Claims to {Fact-Check}: Prioritizing Political
                   Claims Based on {Check-Worthiness}",
  author        = "Kartal, Yavuz Selim and Guvenen, Busra and Kutlu, Mucahid",
  abstract      = "The massive amount of misinformation spreading on the
                   Internet on a daily basis has enormous negative impacts on
                   societies. Therefore, we need automated systems helping
                   fact-checkers in the combat against misinformation. In this
                   paper, we propose a model prioritizing the claims based on
                   their check-worthiness. We use BERT model with additional
                   features including domain-specific controversial topics,
                   word embeddings, and others. In our experiments, we show
                   that our proposed model outperforms all state-of-the-art
                   models in both test collections of CLEF Check That! Lab in
                   2018 and 2019. We also conduct a qualitative analysis to
                   shed light-detecting check-worthy claims. We suggest
                   requesting rationales behind judgments are needed to
                   understand subjective nature of the task and problematic
                   labels.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.08166",
  doi           = "10.48550/arXiv.2004.08166"
}

@INPROCEEDINGS{Gupta2023-jx,
  title     = "Same Same, But Different: Conditional {Multi-Task} Learning for
               {Demographic-Specific} Toxicity Detection",
  booktitle = "Proceedings of the {ACM} Web Conference 2023",
  author    = "Gupta, Soumyajit and Lee, Sooyong and De-Arteaga, Maria and
               Lease, Matthew",
  abstract  = "Algorithmic bias often arises as a result of differential
               subgroup validity, in which predictive relationships vary across
               groups. For example, in toxic language detection, comments
               targeting different demographic groups can vary markedly across
               groups. In such settings, trained models can be dominated by the
               relationships that best fit the majority group, leading to
               disparate performance. We propose framing toxicity detection as
               multi-task learning (MTL), allowing a model to specialize on the
               relationships that are relevant to each demographic group while
               also leveraging shared properties across groups. With toxicity
               detection, each task corresponds to identifying toxicity against
               a particular demographic group. However, traditional MTL
               requires labels for all tasks to be present for every data
               point. To address this, we propose Conditional MTL (CondMTL),
               wherein only training examples relevant to the given demographic
               group are considered by the loss function. This lets us learn
               group specific representations in each branch which are not
               cross contaminated by irrelevant labels. Results on synthetic
               and real data show that using CondMTL improves predictive recall
               over various baselines in general and for the minority
               demographic group in particular, while having similar overall
               accuracy.",
  publisher = "Association for Computing Machinery",
  pages     = "3689--3700",
  series    = "WWW '23",
  month     =  "30~" # apr,
  year      =  2023,
  address   = "New York, NY, USA",
  keywords  = "differential subgroup validity, Multi-task learning, conditional
               loss",
  location  = "Austin, TX, USA",
  isbn      = "9781450394161",
  doi       = "10.1145/3543507.3583290"
}

@INPROCEEDINGS{Hardalov2021-zm,
  title     = "A Survey on Stance Detection for Mis- and Disinformation
               Identification",
  booktitle = "Findings of the Association for Computational Linguistics:
               {NAACL} 2022",
  author    = "Hardalov, Momchil and Arora, Arnav and Nakov, Preslav and
               Augenstein, Isabelle",
  editor    = "Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz,
               Ivan Vladimir",
  abstract  = "Understanding attitudes expressed in texts, also known as stance
               detection, plays an important role in systems for detecting
               false information online, be it misinformation (unintentionally
               false) or disinformation (intentionally false information).
               Stance detection has been framed in different ways, including
               (a) as a component of fact-checking, rumour detection, and
               detecting previously fact-checked claims, or (b) as a task in
               its own right. While there have been prior efforts to contrast
               stance detection with other related tasks such as argumentation
               mining and sentiment analysis, there is no existing survey on
               examining the relationship between stance detection and mis- and
               disinformation detection. Here, we aim to bridge this gap by
               reviewing and analysing existing work in this area, with mis-
               and disinformation in focus, and discussing lessons learnt and
               future challenges.",
  publisher = "Association for Computational Linguistics",
  pages     = "1259--1277",
  month     =  jul,
  year      =  2022,
  address   = "Seattle, United States",
  doi       = "10.18653/v1/2022.findings-naacl.94"
}

@INPROCEEDINGS{Arakelyan2023-ju,
  title     = "{Topic-Guided} Sampling For {Data-Efficient} {Multi-Domain}
               Stance Detection",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  author    = "Arakelyan, Erik and Arora, Arnav and Augenstein, Isabelle",
  editor    = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
  abstract  = "The task of Stance Detection is concerned with identifying the
               attitudes expressed by an author towards a target of interest.
               This task spans a variety of domains ranging from social media
               opinion identification to detecting the stance for a legal
               claim. However, the framing of the task varies within these
               domains in terms of the data collection protocol, the label
               dictionary and the number of available annotations. Furthermore,
               these stance annotations are significantly imbalanced on a
               per-topic and inter-topic basis. These make multi-domain stance
               detection challenging, requiring standardization and domain
               adaptation. To overcome this challenge, we propose Topic
               Efficient StancE Detection (TESTED), consisting of a
               topic-guided diversity sampling technique used for creating a
               multi-domain data efficient training set and a contrastive
               objective that is used for fine-tuning a stance classifier using
               the produced set. We evaluate the method on an existing
               benchmark of 16 datasets with in-domain, i.e. all topics seen
               and out-of-domain, i.e. unseen topics, experiments. The results
               show that the method outperforms the state-of-the-art with an
               average of 3.5 F1 points increase in-domain and is more
               generalizable with an averaged 10.2 F1 on out-of-domain
               evaluation while using \textbackslashtextless10\% of the
               training data. We show that our sampling technique mitigates
               both inter- and per-topic class imbalances. Finally, our
               analysis demonstrates that the contrastive learning objective
               allows the model for a more pronounced segmentation of samples
               with varying labels.",
  publisher = "Association for Computational Linguistics",
  pages     = "13448--13464",
  month     =  jul,
  year      =  2023,
  address   = "Toronto, Canada",
  doi       = "10.18653/v1/2023.acl-long.752"
}

@INPROCEEDINGS{Fan2020-sj,
  title     = "Generating Fact Checking Briefs",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing ({EMNLP})",
  author    = "Fan, Angela and Piktus, Aleksandra and Petroni, Fabio and
               Wenzek, Guillaume and Saeidi, Marzieh and Vlachos, Andreas and
               Bordes, Antoine and Riedel, Sebastian",
  abstract  = "Fact checking at scale is difficult---while the number of active
               fact checking websites is growing, it remains too small for the
               needs of the contemporary media ecosystem. However, despite good
               intentions, contributions from volunteers are often error-prone,
               and thus in practice restricted to claim detection. We
               investigate how to increase the accuracy and efficiency of fact
               checking by providing information about the claim before
               performing the check, in the form of natural language briefs. We
               investigate passage-based briefs, containing a relevant passage
               from Wikipedia, entity-centric ones consisting of Wikipedia
               pages of mentioned entities, and Question-Answering Briefs, with
               questions decomposing the claim, and their answers. To produce
               QABriefs, we develop QABriefer, a model that generates a set of
               questions conditioned on the claim, searches the web for
               evidence, and generates answers. To train its components, we
               introduce QABriefDataset We show that fact checking with briefs
               --- in particular QABriefs --- increases the accuracy of
               crowdworkers by 10\% while slightly decreasing the time taken.
               For volunteer (unpaid) fact checkers, QABriefs slightly increase
               accuracy and reduce the time required by around 20\%.",
  publisher = "Association for Computational Linguistics",
  pages     = "7147--7161",
  month     =  nov,
  year      =  2020,
  address   = "Online",
  doi       = "10.18653/v1/2020.emnlp-main.580"
}

@INPROCEEDINGS{Nguyen2020-we,
  title     = "{FactCatch}: Incremental {Pay-as-You-Go} Fact Checking with
               Minimal User Effort",
  booktitle = "Proceedings of the 43rd International {ACM} {SIGIR} Conference
               on Research and Development in Information Retrieval",
  author    = "Nguyen, Thanh Tam and Weidlich, Matthias and Yin, Hongzhi and
               Zheng, Bolong and Nguyen, Quang Huy and Nguyen, Quoc Viet Hung",
  abstract  = "The open nature of the Web enables users to produce and
               propagate any content without authentication, which has been
               exploited to spread thousands of unverified claims via millions
               of online documents. Maintenance of credible knowledge bases
               thus has to rely on fact checking that constructs a trusted set
               of facts through credibility assessment. Due to an inherent lack
               of ground truth information and language ambiguity, fact
               checking cannot be done in a purely automated manner without
               compromising accuracy. However, state-of-the-art fact checking
               services, rely mostly on human validation, which is costly,
               slow, and non-transparent. This paper presents FactCatch, a
               human-in-the-loop system to guide users in fact checking that
               aims at minimisation of the invested effort. It supports
               incremental quality estimation, mistake mitigation, and
               pay-as-you-go instantiation of a high-quality fact database.",
  publisher = "Association for Computing Machinery",
  pages     = "2165--2168",
  series    = "SIGIR '20",
  month     =  "25~" # jul,
  year      =  2020,
  address   = "New York, NY, USA",
  keywords  = "human-in-the-loop, effort minimisation, fact checking",
  location  = "Virtual Event, China",
  isbn      = "9781450380164",
  doi       = "10.1145/3397271.3401408"
}

@ARTICLE{Chen2023-fu,
  title         = "Complex Claim Verification with Evidence Retrieved in the
                   Wild",
  author        = "Chen, Jifan and Kim, Grace and Sriram, Aniruddh and Durrett,
                   Greg and Choi, Eunsol",
  abstract      = "Evidence retrieval is a core part of automatic
                   fact-checking. Prior work makes simplifying assumptions in
                   retrieval that depart from real-world use cases: either no
                   access to evidence, access to evidence curated by a human
                   fact-checker, or access to evidence available long after the
                   claim has been made. In this work, we present the first
                   fully automated pipeline to check real-world claims by
                   retrieving raw evidence from the web. We restrict our
                   retriever to only search documents available prior to the
                   claim's making, modeling the realistic scenario where an
                   emerging claim needs to be checked. Our pipeline includes
                   five components: claim decomposition, raw document
                   retrieval, fine-grained evidence retrieval, claim-focused
                   summarization, and veracity judgment. We conduct experiments
                   on complex political claims in the ClaimDecomp dataset and
                   show that the aggregated evidence produced by our pipeline
                   improves veracity judgments. Human evaluation finds the
                   evidence summary produced by our system is reliable (it does
                   not hallucinate information) and relevant to answering key
                   questions about a claim, suggesting that it can assist
                   fact-checkers even when it cannot surface a complete
                   evidence set.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.11859",
  doi           = "10.48550/arXiv.2305.11859"
}

@MISC{Saeed2021-ff,
  title        = "Fact-checking statistical claims with tables",
  author       = "Saeed, Mohammed and Papotti, Paolo",
  abstract     = "… of fact - checking a statistical claim with relational
                  tables as reference data and considered four prominent
                  systems. We make a first step towards categorizing fact -
                  checking systems with …",
  publisher    = "eurecom.fr",
  year         =  2021,
  howpublished = "\url{https://www.eurecom.fr/publication/6630/download/data-publi-6630.pdf}",
  note         = "Accessed: 2023-6-14"
}

@ARTICLE{Karagiannis2020ScrutinizerAM,
  title     = "Scrutinizer: fact checking statistical claims",
  author    = "Karagiannis, Georgios and Saeed, Mohammed and Papotti, Paolo and
               Trummer, Immanuel",
  abstract  = "We demonstrate Scrutinizer, a system that supports human fact
               checkers in translating text claims into SQL queries on an
               associated database. Scrutinizer coordinates teams of human fact
               checkers and reduces their verification time by proposing
               queries or query fragments over relevant data. Those proposals
               are based on claim text classifiers, that gradually improve
               during the verification of multiple claims. In addition,
               Scrutinizer uses tentative execution of query candidates to
               narrow down the set of alternatives. The verification process is
               controlled by a cost-based optimizer that plans effective
               question sequences to verify specific claims, and prioritizes
               claims for verification. In this demonstration, we first show
               how our system can assist users in verifying statistical claims.
               We then let users come up with new, unseen claims and show how
               the system effectively learns new queries with little user
               feedback.",
  journal   = "Proceedings of the VLDB Endowment International Conference on
               Very Large Data Bases",
  publisher = "VLDB Endowment",
  volume    =  13,
  number    =  12,
  pages     = "2965--2968",
  month     =  "1~" # aug,
  year      =  2020,
  issn      = "2150-8097",
  doi       = "10.14778/3415478.3415520"
}

@INPROCEEDINGS{Balalau2022-wu,
  title     = "{Fact-checking Multidimensional Statistic Claims in French}",
  booktitle = "{TTO} 2022-Truth and Trust Online",
  author    = "Balalau, Oana and Ebel, Simon and Galizzi, Th{\'e}o and
               Manolescu, Ioana and Massonnat, Quentin and Deiana, Antoine and
               Gautreau, Emilie and Krempf, Antoine and Pontillon, Thomas and
               Roux, G{\'e}rald and {Others}",
  year      =  2022,
  doi       = "hal.science/hal-03791175/document"
}

@INPROCEEDINGS{Zhang2021-mh,
  title     = "Detecting and Forecasting Misinformation via Temporal and
               Geometric Propagation Patterns",
  booktitle = "Advances in Information Retrieval",
  author    = "Zhang, Qiang and Cook, Jonathan and Yilmaz, Emine",
  abstract  = "Misinformation takes the form of a false claim under the guise
               of fact. It is necessary to protect social media against
               misinformation by means of effective misinformation detection
               and analysis. To this end, we formulate misinformation
               propagation as a dynamic graph, then extract the temporal
               evolution patterns and geometric features of the propagation
               graph based on Temporal Point Processes (TPPs). TPPs provide the
               appropriate modelling framework for a list of stochastic,
               discrete events. In this context, that is a sequence of social
               user engagements. Furthermore, we forecast the cumulative number
               of engaged users based on a power law. Such forecasting
               capabilities can be useful in assessing the threat level of
               misinformation pieces. By jointly considering the geometric and
               temporal propagation patterns, our model has achieved comparable
               performance with state-of-the-art baselines on two well known
               datasets.",
  publisher = "Springer International Publishing",
  pages     = "455--462",
  year      =  2021,
  doi       = "10.1007/978-3-030-72240-1_48"
}

@INPROCEEDINGS{Vo2019-mr,
  title     = "Learning from Fact-checkers: Analysis and Generation of
               Fact-checking Language",
  booktitle = "Proceedings of the 42nd International {ACM} {SIGIR} Conference
               on Research and Development in Information Retrieval",
  author    = "Vo, Nguyen and Lee, Kyumin",
  abstract  = "In fighting against fake news, many fact-checking systems
               comprised of human-based fact-checking sites (e.g., snopes.com
               and politifact.com) and automatic detection systems have been
               developed in recent years. However, online users still keep
               sharing fake news even when it has been debunked. It means that
               early fake news detection may be insufficient and we need
               another complementary approach to mitigate the spread of
               misinformation. In this paper, we introduce a novel application
               of text generation for combating fake news. In particular, we
               (1) leverage online users named fact-checkers, who cite
               fact-checking sites as credible evidences to fact-check
               information in public discourse; (2) analyze linguistic
               characteristics of fact-checking tweets; and (3) propose and
               build a deep learning framework to generate responses with
               fact-checking intention to increase the fact-checkers'
               engagement in fact-checking activities. Our analysis reveals
               that the fact-checkers tend to refute misinformation and use
               formal language (e.g. few swear words and Internet slangs). Our
               framework successfully generates relevant responses, and
               outperforms competing models by achieving up to 30\%
               improvements. Our qualitative study also confirms that the
               superiority of our generated responses compared with responses
               generated from the existing models.",
  publisher = "Association for Computing Machinery",
  pages     = "335--344",
  series    = "SIGIR'19",
  month     =  "18~" # jul,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "text generation, fake news, fact-checking",
  location  = "Paris, France",
  isbn      = "9781450361729",
  doi       = "10.1145/3331184.3331248"
}